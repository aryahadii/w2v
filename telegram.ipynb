{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('msgs.json') as msgs_file:\n",
    "    msgs = json.load(msgs_file)\n",
    "    \n",
    "msgs['chats']['list'] += msgs['left_chats']['list']\n",
    "msgs = msgs['chats']['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [m for m in msgs if m['type'] not in ['saved_messages']]\n",
    "threads = {m['name']: m['messages'] for m in msgs}\n",
    "\n",
    "print(f'Threads: {len(threads)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs_count = 0\n",
    "for name, messages in threads.items():\n",
    "    msgs_count += len(messages)\n",
    "print(f'Messages: {msgs_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs_type = set()\n",
    "for _, messages in threads.items():\n",
    "    for message in messages:\n",
    "        msgs_type.add(message['type'])\n",
    "print(f'Message types: {msgs_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_thread_messages(msgs):\n",
    "    merged_msgs = []\n",
    "    \n",
    "    last_writer = None\n",
    "    last_msg = ''\n",
    "    for msg in msgs:\n",
    "        if msg['type'] != 'message':\n",
    "            continue\n",
    "        if 'from' not in msg:\n",
    "            continue\n",
    "        if type(msg['text']) is not str:\n",
    "            continue\n",
    "\n",
    "        if msg['from'] == last_writer:\n",
    "            last_msg += f\" {msg['text']}\"\n",
    "        else:\n",
    "            merged_msgs.append(last_msg)\n",
    "            last_writer = msg['from']\n",
    "            last_msg = msg['text']\n",
    "\n",
    "    return merged_msgs\n",
    "        \n",
    "merged_msgs = {n: merge_thread_messages(m) for n, m in threads.items()}\n",
    "print(f'Messages length: {len(merged_msgs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [sen for thread in merged_msgs.values() for sen in thread]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, WordTokenizer, Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "msgs = [normalizer.normalize(s) for s in msgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def stopwords_list(stopwords_file):\n",
    "    with codecs.open(stopwords_file, encoding='utf8') as stopwords_file:\n",
    "        return list(map(lambda w: w.strip(), stopwords_file))\n",
    "\n",
    "stopwords = set(stopwords_list(\"stopwords.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = WordTokenizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "msgs = [[lemmatizer.lemmatize(w) for w in word_tokenizer.tokenize(s) if w not in stopwords] for s in msgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(msgs, size=100, sg=1, iter=20, min_count=30, workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_viz import visualize_notebook, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for idx, word in enumerate(model.wv.vocab):\n",
    "    words_dict[word] = idx\n",
    "print(f'Words: {len(words_dict.keys())}')\n",
    "\n",
    "edges = []\n",
    "edge_count = {}\n",
    "for word in words_dict:\n",
    "    for other_word, similarity in model.wv.similar_by_word(word):\n",
    "        if similarity < 0.55:\n",
    "            break\n",
    "        if other_word not in words_dict:\n",
    "            continue\n",
    "        if word not in edge_count:\n",
    "            edge_count[word] = 0\n",
    "        edge_count[word] += 1\n",
    "        if other_word not in edge_count:\n",
    "            edge_count[other_word] = 0\n",
    "        edge_count[other_word] += 1\n",
    "        edges.append((word, other_word, 1))\n",
    "\n",
    "new_edges = []\n",
    "for word, other_word, w in edges:\n",
    "    if edge_count[word] < 3 or edge_count[other_word] < 3:\n",
    "        continue\n",
    "    new_edges.append((word, other_word, w))\n",
    "edges = new_edges\n",
    "\n",
    "html = visualize(edges, size=800)\n",
    "with open('output.html','w') as output_file:\n",
    "    output_file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
